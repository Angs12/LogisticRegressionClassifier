{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92791,"databundleVersionId":11083833,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport nltk\n\ncsv_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\n\narr = csv_data.to_numpy()\n\ndef preprocess_data(arr):\n    link_user_regex = re.compile(r'(www\\.\\w*\\.\\w*)|(@\\w*)|(https?://(\\w|[./])*)\\s*') #remove links and usernames\n    remove_html_regex = re.compile(r'&\\w*(;|\\b)') #remove html symbols\n    remove_nonascii_regex = re.compile(r'[^ -~]+')\n    remove_symbols_regex = re.compile(r'[\\|{}_,/.\\'\\\"`<>!\\\\@#$%^\\(\\)\\*\\-=\\+/\\?;:~\\[\\]]+') #remove symbols\n    remove_numbers_regex = re.compile(r'(\\b[a-zA-Z]\\b)|([0-9]+)') #remove numbers/letters\n    ty_regex = re.compile(r'\\bty\\b')\n    num_regex = re.compile(r'\\b(one|two|three|four|five|sic|seven|eight|nine|ten)\\b')\n    good_regex = re.compile(r'\\bnice\\b')\n    stopwrd = ('of', 'in', 'for', 'is', 'you', 'and', 'my', 'it', 'to')\n    stopwrd_dict = Counter(stopwrd)\n    for i in range(len(arr)):\n        row = arr[i]\n        row[1] = remove_nonascii_regex.sub(\" \",row[1])\n        row[1] = link_user_regex.sub(\" \",row[1])\n        row[1] = remove_html_regex.sub(\" \",row[1])\n        row[1] = remove_symbols_regex.sub(\" \",row[1])\n        row[1] = remove_numbers_regex.sub(\" \",row[1])\n        row[1] = ' '.join([word for word in row[1].split() if word.lower() not in stopwrd_dict])\n        row[1] = ty_regex.sub(\"thank\",row[1])\n        row[1] = good_regex.sub(\"good\",row[1])\n        row[1] = num_regex.sub(\" \",row[1])\n    return arr\n\ndef stemmer(arr):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    for i in range(len(arr)):\n        row = arr[i]\n        sentence = []\n        for word in row[1].split():\n            sentence.append(stemmer.stem(word))\n        row[1] = \" \".join(sentence)\n    return arr\n\ndef remove_empty(arr):\n    new_arr = []\n    for i in range(len(arr)):\n        if arr[i][1] == \"\":\n            continue\n        new_arr.append(arr[i])\n    return np.array(new_arr)\n\n\narr = preprocess_data(arr)\narr = stemmer(arr)\n\npos_count = 0\nfor i in range(len(arr)):\n    #print(arr[i][1])\n    pos_count += arr[i][2]\n\nL_pos = []\nL_neg = []\nfor i in range(len(arr)):\n    if arr[i][2] == 1:\n        L_pos += arr[i][1].split()\n    else:\n        L_neg += arr[i][1].split()\nwords_pos = np.array(L_pos)\nwords_neg = np.array(L_neg)\n\n(words_pos,counts_pos) = np.unique(words_pos,return_counts=True)\n(words_neg,counts_neg) = np.unique(words_neg,return_counts=True)\npos_pop_words = sorted(list(zip(words_pos,counts_pos)),key= lambda tuple: tuple[1])\nneg_pop_words = sorted(list(zip(words_neg,counts_neg)),key= lambda tuple: tuple[1])\nwords_pos,counts_pos = zip(*pos_pop_words[-10:])\nwords_neg,counts_neg = zip(*neg_pop_words[-10:])\nplt.figure()\nplt.bar(np.arange(10)-0.2,counts_pos,0.4,color=\"blue\")\nplt.bar(np.arange(10)+0.2,counts_neg,0.4,color=\"orange\")\nplt.title(\"Popular Words\")\nplt.xticks(np.arange(10),words_pos)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/pop_words.png\")\n\nneg_count = len(arr) - pos_count\nplt.figure()\nplt.bar([\"Negative\",\"Positive\"],[neg_count,pos_count])\nplt.title(\"Sentiment Count\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/sentiment_count.png\")","metadata":{"_uuid":"5ac2031c-358b-48bd-bb62-b81acd82c8c6","_cell_guid":"8673f056-20fc-4862-86bc-4091efba2d6d","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T23:25:44.417023Z","iopub.execute_input":"2025-02-26T23:25:44.417295Z","iopub.status.idle":"2025-02-26T23:25:44.441837Z","shell.execute_reply.started":"2025-02-26T23:25:44.417274Z","shell.execute_reply":"2025-02-26T23:25:44.440612Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-7292c9c8484f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwnl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'us'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcsv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\nimport sklearn.metrics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\ntrain_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\nval_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/val_dataset.csv\")\n\ndef load_data(data):\n    arr = data.to_numpy()\n    arr = preprocess_data(arr)\n    arr = stemmer(arr)\n    return arr\n\ndef create_input(vectorizer,data):\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.transform(tweets)\n    return (features,gold_labels)\n\ndef vectorize_input(vectorizer,data):\n    tweets = data[:,1]\n    features = vectorizer.transform(tweets)\n    return features\n\ndef train_model(vectorizer,data):\n    random.seed()\n    rng = np.random.RandomState(random.randint(0,1000000))\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    #log_regr = LogisticRegression()\n    #param_grid = [{\n    #    'penalty' : ['elasticnet'],\n    #    'solver': ['saga'],\n    #    'l1_ratio': np.linspace(0,1,20)\n    #}]\n    #clf = GridSearchCV(log_regr,param_grid = param_grid,n_jobs=-1,cv=3)\n    #print(clf.best_estimator_)\n    clf = LogisticRegression(random_state=rng,n_jobs=-1,solver='saga').fit(features,gold_labels)\n    return clf\n\ndef create_learning_curve(clf,vectorizer,data):\n    tweets = data[:,1]\n    labels = data[:,2]\n    labels = labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    train_size, train_scores, test_scores = learning_curve(clf,features,labels,cv=3)\n    train_mean = np.mean(train_scores,axis=1)\n    test_mean = np.mean(test_scores,axis=1)\n    plt.plot(train_size,train_mean,color='blue')\n    plt.plot(train_size,test_mean,color='orange')\n    plt.xlabel('Data size')\n    plt.ylabel('Score')\n    plt.legend(['Train Score', 'Test Score'])\n    plt.savefig(\"/kaggle/working/learning_curve.png\")\n    \n\ntrain = load_data(train_data)\ntrain = remove_empty(train)\nval = load_data(val_data)\n\nvectorizer = TfidfVectorizer(lowercase=False,strip_accents='ascii',sublinear_tf=True,ngram_range = (1,2))\nclf = train_model(vectorizer,train)\n\nfeatures_val,val_gold_labels = create_input(vectorizer,val)\n\npred_labels = clf.predict(features_val)\n\nprint(\"F1: \",f1_score(val_gold_labels,pred_labels))\nprint(\"Accuracy: \",accuracy_score(val_gold_labels,pred_labels))\nprint(\"Precision: \",precision_score(val_gold_labels,pred_labels))\nprint(\"Recall: \",recall_score(val_gold_labels,pred_labels))\n\nclf = LogisticRegression(n_jobs=-1,solver='saga')\ncreate_learning_curve(clf,vectorizer,train)","metadata":{"_uuid":"5d327e13-e85d-4e54-832f-3b211e4786c9","_cell_guid":"fd5d3d66-f8d2-4d2a-9d0f-5e7110766b7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-26T23:19:52.915053Z","iopub.status.idle":"2025-02-26T23:19:52.915263Z","shell.execute_reply":"2025-02-26T23:19:52.915181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/test_dataset.csv\")\n\ntrain = load_data(train_data)\ntrain = remove_empty(train)\ntest = load_data(test_data)\n\nvectorizer = TfidfVectorizer(lowercase=False,strip_accents='ascii',sublinear_tf=True,ngram_range = (1,2))\n\nclf = train_model(vectorizer,train)\ntest_features = vectorize_input(vectorizer,test)\n\npred_labels = clf.predict(test_features)\n\nL = []\nfor i in range(len(pred_labels)):\n    L.append([test[i,0],pred_labels[i]])\n\narr = np.array(L)\ncolumns = ['ID','Label']\ndataframe = pd.DataFrame(data=arr,columns=columns)\n\ndataframe.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T23:19:52.915768Z","iopub.status.idle":"2025-02-26T23:19:52.916014Z","shell.execute_reply":"2025-02-26T23:19:52.915924Z"}},"outputs":[],"execution_count":null}]}