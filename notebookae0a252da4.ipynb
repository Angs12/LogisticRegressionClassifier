{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92791,"databundleVersionId":11083833,"sourceType":"competition"},{"sourceId":10813220,"sourceType":"datasetVersion","datasetId":6709477}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\n\ncsv_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\n\narr = csv_data.to_numpy()\n\ndef preprocess_data(arr):\n    link_user_regex = re.compile(r'(@\\w*)|(https?://(\\w|[./])*)\\s*') #remove links and usernames\n    remove_html_regex = re.compile(r'&\\w*;?') #remove html symbols\n    remove_symbols_regex = re.compile(r'[|{}_,/.\\'\\\"`<>!\\\\@#$%^\\(\\)*\\-=\\+/?;:~\\[\\]]+') #remove symbols\n    remove_numbers_regex = re.compile(r'(\\b[a-zA-Z]\\b)|([0-9]+)') #remove numbers/letters\n    not_ascii_regex = re.compile(r'[^ -~]') #remove non ascii chars\n    for i in range(len(arr)):\n        row = arr[i]\n        row[1] = link_user_regex.sub(\"\",row[1])\n        row[1] = remove_html_regex.sub(\"\",row[1])\n        row[1] = remove_symbols_regex.sub(\" \",row[1])\n        row[1] = not_ascii_regex.sub(\"\",row[1])\n        row[1] = remove_numbers_regex.sub(\"\",row[1])\n    return arr\n\ndef remove_stopwords(arr):\n    stopwords_file = \"/kaggle/input/eng-stopwords/eng_stopwords.txt\"\n    stopwords = open(stopwords_file,\"r\")\n    stopwords = stopwords.read().splitlines()\n    regex = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n    for i in range(len(arr)):\n        row = arr[i]\n        row[1] = regex.sub(\"\",row[1])\n    return arr\n\ndef stemmer(arr):\n    stemmer = nltk.stem.PorterStemmer()\n    for i in range(len(arr)):\n        row = arr[i]\n        sentence = []\n        for word in row[1].split():\n            sentence.append(stemmer.stem(word))\n        row[1] = \" \".join(sentence)\n    return arr\n\narr = preprocess_data(arr)\narr = remove_stopwords(arr)\narr = stemmer(arr)\n\npos_count = 0\nfor i in range(len(arr)):\n    pos_count += arr[i][2]\n\nL = []\nfor i in range(len(arr)):\n    L += arr[i][1].split()\nwords = np.array(L)\n\n(words,counts) = np.unique(words,return_counts=True)\npop_words = sorted(list(zip(words,counts)),key= lambda tuple: tuple[1])\nwords,counts = zip(*pop_words[-10:])\n\nplt.figure()\nplt.bar(words,counts)\nplt.title(\"Popular Words\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/pop_words.png\")\n\nneg_count = len(arr) - pos_count\nplt.figure()\nplt.bar([\"Negative\",\"Positive\"],[neg_count,pos_count])\nplt.title(\"Sentiment Count\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/sentiment_count.png\")","metadata":{"_uuid":"5ac2031c-358b-48bd-bb62-b81acd82c8c6","_cell_guid":"8673f056-20fc-4862-86bc-4091efba2d6d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-25T23:59:38.549750Z","iopub.execute_input":"2025-02-25T23:59:38.550101Z","iopub.status.idle":"2025-02-25T23:59:40.466992Z","shell.execute_reply.started":"2025-02-25T23:59:38.550074Z","shell.execute_reply":"2025-02-25T23:59:40.465457Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-490c38d9e596>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-490c38d9e596>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_user_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_html_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_symbols_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\nimport sklearn.metrics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\ntrain_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\nval_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/val_dataset.csv\")\n\ndef load_data(data):\n    arr = data.to_numpy()\n    arr = preprocess_data(arr)\n    #arr = remove_stopwords(arr)\n    arr = stemmer(arr)\n    return arr\n\ndef create_input(vectorizer,data):\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.transform(tweets)\n    return (features,gold_labels)\n\ndef vectorize_input(vectorizer,data):\n    tweets = data[:,1]\n    features = vectorizer.transform(tweets)\n    return features\n\ndef train_model(vectorizer,data):\n    random.seed()\n    rng = np.random.RandomState(random.randint(0,1000000))\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    #log_regr = LogisticRegression()\n    #param_grid = [{\n    #    'penalty' : ['elasticnet'],\n    #    'solver': ['saga'],\n    #    'l1_ratio': np.linspace(0,1,20)\n    #}]\n    #clf = GridSearchCV(log_regr,param_grid = param_grid,n_jobs=-1,cv=3)\n    #print(clf.best_estimator_)\n    clf = LogisticRegression(random_state=rng,n_jobs=-1,solver='saga').fit(features,gold_labels)\n    return clf\n\ndef create_learning_curve(clf,vectorizer,data):\n    tweets = data[:,1]\n    labels = data[:,2]\n    labels = labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    train_size, train_scores, test_scores = learning_curve(clf,features,labels,cv=3)\n    train_mean = np.mean(train_scores,axis=1)\n    test_mean = np.mean(test_scores,axis=1)\n    plt.plot(train_size,train_mean,color='blue')\n    plt.plot(train_size,test_mean,color='orange')\n    plt.xlabel('Data size')\n    plt.ylabel('Score')\n    plt.legend(['Train Score', 'Test Score'])\n    plt.show()\n    plt.savefig(\"/kaggle/working/learning_curve.png\")\n    \n\ntrain = load_data(train_data)\nval = load_data(val_data)\n\nvectorizer = TfidfVectorizer(lowercase=True)\nclf = train_model(vectorizer,train)\n\nfeatures_val,val_gold_labels = create_input(vectorizer,val)\n\npred_labels = clf.predict(features_val)\n\nprint(\"F1: \",f1_score(val_gold_labels,pred_labels))\nprint(\"Accuracy: \",accuracy_score(val_gold_labels,pred_labels))\nprint(\"Precision: \",precision_score(val_gold_labels,pred_labels))\nprint(\"Recall: \",recall_score(val_gold_labels,pred_labels))\n\nclf = LogisticRegression(n_jobs=-1,solver='saga')\ncreate_learning_curve(clf,vectorizer,train)","metadata":{"_uuid":"5d327e13-e85d-4e54-832f-3b211e4786c9","_cell_guid":"fd5d3d66-f8d2-4d2a-9d0f-5e7110766b7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-26T00:02:54.698916Z","iopub.execute_input":"2025-02-26T00:02:54.699403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/test_dataset.csv\")\n\ntrain = load_data(train_data)\ntest = load_data(test_data)\n\nvectorizer = TfidfVectorizer()\n\nclf = train_model(vectorizer,train)\ntest_features = vectorize_input(vectorizer,test)\n\npred_labels = clf.predict(test_features)\n\nL = []\nfor i in range(len(pred_labels)):\n    L.append([test[i,0],pred_labels[i]])\n\narr = np.array(L)\ncolumns = ['ID','Label']\ndataframe = pd.DataFrame(data=arr,columns=columns)\n\ndataframe.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:07:51.788436Z","iopub.execute_input":"2025-02-24T12:07:51.788822Z","iopub.status.idle":"2025-02-24T12:08:24.493177Z","shell.execute_reply.started":"2025-02-24T12:07:51.788794Z","shell.execute_reply":"2025-02-24T12:08:24.491998Z"}},"outputs":[],"execution_count":null}]}