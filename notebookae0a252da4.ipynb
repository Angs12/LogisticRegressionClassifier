{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92791,"databundleVersionId":11083833,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport nltk\n\ncsv_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\n\narr = csv_data.to_numpy()\n\ndef preprocess_data(arr):\n    link_user_regex = re.compile(r'(www\\.\\w*\\.\\w*)|(@\\w*)|(https?://(\\w|[./])*)\\s*') #remove links and usernames\n    remove_html_regex = re.compile(r'&\\w*(;|\\b)') #remove html symbols\n    remove_nonascii_regex = re.compile(r'[^ -~]+')\n    remove_symbols_regex = re.compile(r'[\\|{}_,/.\\'\\\"`<>!\\\\@#$%^\\(\\)\\*\\-=\\+/\\?;:~\\[\\]]+') #remove symbols\n    remove_numbers_regex = re.compile(r'(\\b[a-zA-Z]\\b)|([0-9]+)') #remove numbers/letters\n    remove_hashtags_regex = re.compile(r'\\b#\\w+\\b')\n    ty_regex = re.compile(r'\\bty\\b')\n    num_regex = re.compile(r'\\b(one|two|three|four|five|sic|seven|eight|nine|ten)\\b')\n    good_regex = re.compile(r'\\bnice\\b')\n    stopwrd = ('of', 'in', 'for', 'is', 'you', 'and', 'my', 'it', 'to')\n    stopwrd_dict = Counter(stopwrd)\n    for i in range(len(arr)):\n        row = arr[i]\n        row[1] = remove_nonascii_regex.sub(\" \",row[1])\n        row[1] = remove_hashtags_regex.sub(\" \",row[1])\n        row[1] = link_user_regex.sub(\" \",row[1])\n        row[1] = remove_html_regex.sub(\" \",row[1])\n        row[1] = remove_symbols_regex.sub(\" \",row[1])\n        row[1] = remove_numbers_regex.sub(\" \",row[1])\n        row[1] = ' '.join([word for word in row[1].split() if word.lower() not in stopwrd_dict])\n        row[1] = ty_regex.sub(\"thank\",row[1])\n        row[1] = good_regex.sub(\"good\",row[1])\n        row[1] = num_regex.sub(\" \",row[1])\n    return arr\n\ndef stemmer(arr):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    for i in range(len(arr)):\n        row = arr[i]\n        sentence = []\n        for word in row[1].split():\n            sentence.append(stemmer.stem(word))\n        row[1] = \" \".join(sentence)\n    return arr\n\ndef remove_empty(arr):\n    new_arr = []\n    for i in range(len(arr)):\n        if arr[i][1] == \"\":\n            continue\n        new_arr.append(arr[i])\n    return np.array(new_arr)\n\n\narr = preprocess_data(arr)\narr = stemmer(arr)\n\npos_count = 0\nfor i in range(len(arr)):\n    #print(arr[i][1])\n    pos_count += arr[i][2]\n\nL_pos = []\nL_neg = []\nfor i in range(len(arr)):\n    if arr[i][2] == 1:\n        L_pos += arr[i][1].split()\n    else:\n        L_neg += arr[i][1].split()\nwords_pos = np.array(L_pos)\nwords_neg = np.array(L_neg)\n\n(words_pos,counts_pos) = np.unique(words_pos,return_counts=True)\n(words_neg,counts_neg) = np.unique(words_neg,return_counts=True)\npos_pop_words = sorted(list(zip(words_pos,counts_pos)),key= lambda tuple: tuple[1])\nneg_pop_words = sorted(list(zip(words_neg,counts_neg)),key= lambda tuple: tuple[1])\nwords_pos,counts_pos = zip(*pos_pop_words[-10:])\nwords_neg,counts_neg = zip(*neg_pop_words[-10:])\nplt.figure()\nplt.bar(np.arange(10)-0.2,counts_pos,0.4,color=\"blue\")\nplt.bar(np.arange(10)+0.2,counts_neg,0.4,color=\"orange\")\nplt.title(\"Popular Words\")\nplt.xticks(np.arange(10),words_pos)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/pop_words.png\")\n\nneg_count = len(arr) - pos_count\nplt.figure()\nplt.bar([\"Negative\",\"Positive\"],[neg_count,pos_count])\nplt.title(\"Sentiment Count\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.savefig(\"/kaggle/working/sentiment_count.png\")","metadata":{"_uuid":"5ac2031c-358b-48bd-bb62-b81acd82c8c6","_cell_guid":"8673f056-20fc-4862-86bc-4091efba2d6d","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:11:37.478275Z","iopub.execute_input":"2025-02-27T11:11:37.478724Z","iopub.status.idle":"2025-02-27T11:12:03.619416Z","shell.execute_reply.started":"2025-02-27T11:11:37.478691Z","shell.execute_reply":"2025-02-27T11:12:03.618213Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\nimport sklearn.metrics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\ntrain_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\nval_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/val_dataset.csv\")\n\ndef load_data(data):\n    arr = data.to_numpy()\n    arr = preprocess_data(arr)\n    arr = stemmer(arr)\n    return arr\n\ndef create_input(vectorizer,data):\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.transform(tweets)\n    return (features,gold_labels)\n\ndef vectorize_input(vectorizer,data):\n    tweets = data[:,1]\n    features = vectorizer.transform(tweets)\n    return features\n\ndef train_model(vectorizer,data):\n    random.seed()\n    rng = np.random.RandomState(random.randint(0,1000000))\n    tweets = data[:,1]\n    gold_labels = data[:,2]\n    gold_labels = gold_labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    #log_regr = LogisticRegression()\n    #param_grid = [{\n    #    'penalty' : ['elasticnet'],\n    #    'solver': ['saga'],\n    #    'l1_ratio': np.linspace(0,1,20)\n    #}]\n    #clf = GridSearchCV(log_regr,param_grid = param_grid,n_jobs=-1,cv=3)\n    #print(clf.best_estimator_)\n    clf = LogisticRegression(random_state=rng,n_jobs=-1,solver='saga').fit(features,gold_labels)\n    return clf\n\ndef create_learning_curve(clf,vectorizer,data):\n    tweets = data[:,1]\n    labels = data[:,2]\n    labels = labels.astype('int')\n    features = vectorizer.fit_transform(tweets)\n    train_size, train_scores, test_scores = learning_curve(clf,features,labels,cv=3)\n    train_mean = np.mean(train_scores,axis=1)\n    test_mean = np.mean(test_scores,axis=1)\n    plt.plot(train_size,train_mean,color='blue')\n    plt.plot(train_size,test_mean,color='orange')\n    plt.xlabel('Data size')\n    plt.ylabel('Score')\n    plt.legend(['Train Score', 'Test Score'])\n    plt.savefig(\"/kaggle/working/learning_curve.png\")\n    \n\ntrain = load_data(train_data)\ntrain = remove_empty(train)\nval = load_data(val_data)\n\nvectorizer = TfidfVectorizer(lowercase=False,strip_accents='ascii',sublinear_tf=True,ngram_range = (1,3),min_df=2)\n\nclf = train_model(vectorizer,train)\n\nfeatures_val,val_gold_labels = create_input(vectorizer,val)\n\npred_labels = clf.predict(features_val)\n\nprint(\"F1: \",f1_score(val_gold_labels,pred_labels))\nprint(\"Accuracy: \",accuracy_score(val_gold_labels,pred_labels))\nprint(\"Precision: \",precision_score(val_gold_labels,pred_labels))\nprint(\"Recall: \",recall_score(val_gold_labels,pred_labels))\n\nclf = LogisticRegression(n_jobs=-1,solver='saga')\ncreate_learning_curve(clf,vectorizer,train)","metadata":{"_uuid":"5d327e13-e85d-4e54-832f-3b211e4786c9","_cell_guid":"fd5d3d66-f8d2-4d2a-9d0f-5e7110766b7c","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:24:14.409447Z","iopub.execute_input":"2025-02-27T12:24:14.409897Z","iopub.status.idle":"2025-02-27T12:25:24.672167Z","shell.execute_reply.started":"2025-02-27T12:24:14.409864Z","shell.execute_reply":"2025-02-27T12:25:24.670615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/test_dataset.csv\")\n\ntrain = load_data(train_data)\ntrain = remove_empty(train)\ntest = load_data(test_data)\n\nvectorizer = TfidfVectorizer(lowercase=False,strip_accents='ascii',sublinear_tf=True,ngram_range = (1,3),min_df=2)\n\nclf = train_model(vectorizer,train)\ntest_features = vectorize_input(vectorizer,test)\n\npred_labels = clf.predict(test_features)\n\nL = []\nfor i in range(len(pred_labels)):\n    L.append([test[i,0],pred_labels[i]])\n\narr = np.array(L)\ncolumns = ['ID','Label']\ndataframe = pd.DataFrame(data=arr,columns=columns)\n\ndataframe.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T23:19:52.915768Z","iopub.status.idle":"2025-02-26T23:19:52.916014Z","shell.execute_reply":"2025-02-26T23:19:52.915924Z"}},"outputs":[],"execution_count":null}]}